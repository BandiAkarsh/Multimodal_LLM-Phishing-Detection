{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Staged Data Processing: Multimodal Feature Extraction\n",
    "This notebook processes the dataset in strategic batches:\n",
    "1. Filter active URLs\n",
    "2. Balance dataset (phishing + legitimate)\n",
    "3. Process in batches to avoid memory issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# Add utils to path\n",
    "sys.path.append(os.path.abspath('../05_utils'))\n",
    "from data_preparation import DataPreprocessor\n",
    "\n",
    "print(\"✅ Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "with open('../07_configs/config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"✅ Configuration loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Analyze Current Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load splits\n",
    "train_df = pd.read_csv('../01_data/splits/train.csv')\n",
    "val_df = pd.read_csv('../01_data/splits/val.csv')\n",
    "test_df = pd.read_csv('../01_data/splits/test.csv')\n",
    "\n",
    "print(\"Dataset Statistics:\")\n",
    "print(f\"Train: {len(train_df)} samples\")\n",
    "print(f\"  - Phishing: {sum(train_df['label'] == 1)}\")\n",
    "print(f\"  - Legitimate: {sum(train_df['label'] == 0)}\")\n",
    "print(f\"\\nValidation: {len(val_df)} samples\")\n",
    "print(f\"  - Phishing: {sum(val_df['label'] == 1)}\")\n",
    "print(f\"  - Legitimate: {sum(val_df['label'] == 0)}\")\n",
    "print(f\"\\nTest: {len(test_df)} samples\")\n",
    "print(f\"  - Phishing: {sum(test_df['label'] == 1)}\")\n",
    "print(f\"  - Legitimate: {sum(test_df['label'] == 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Add More Legitimate URLs\n",
    "**Problem:** Your dataset is 99.6% phishing. We need more legitimate samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add legitimate URLs from popular domains\n",
    "legitimate_urls = [\n",
    "    # Tech companies\n",
    "    \"https://www.google.com\",\n",
    "    \"https://www.github.com\",\n",
    "    \"https://www.microsoft.com\",\n",
    "    \"https://www.apple.com\",\n",
    "    \"https://www.amazon.com\",\n",
    "    \"https://www.facebook.com\",\n",
    "    \"https://www.twitter.com\",\n",
    "    \"https://www.linkedin.com\",\n",
    "    \"https://www.instagram.com\",\n",
    "    \"https://www.youtube.com\",\n",
    "    \n",
    "    # News & Media\n",
    "    \"https://www.cnn.com\",\n",
    "    \"https://www.bbc.com\",\n",
    "    \"https://www.nytimes.com\",\n",
    "    \"https://www.theguardian.com\",\n",
    "    \"https://www.reuters.com\",\n",
    "    \n",
    "    # Education\n",
    "    \"https://www.wikipedia.org\",\n",
    "    \"https://www.stackoverflow.com\",\n",
    "    \"https://www.coursera.org\",\n",
    "    \"https://www.edx.org\",\n",
    "    \"https://www.khanacademy.org\",\n",
    "    \n",
    "    # E-commerce\n",
    "    \"https://www.ebay.com\",\n",
    "    \"https://www.walmart.com\",\n",
    "    \"https://www.target.com\",\n",
    "    \"https://www.bestbuy.com\",\n",
    "    \n",
    "    # Finance (legitimate)\n",
    "    \"https://www.paypal.com\",\n",
    "    \"https://www.chase.com\",\n",
    "    \"https://www.bankofamerica.com\",\n",
    "    \"https://www.wellsfargo.com\",\n",
    "    \n",
    "    # Government\n",
    "    \"https://www.usa.gov\",\n",
    "    \"https://www.irs.gov\",\n",
    "    \"https://www.nasa.gov\",\n",
    "    \n",
    "    # Universities\n",
    "    \"https://www.mit.edu\",\n",
    "    \"https://www.stanford.edu\",\n",
    "    \"https://www.harvard.edu\",\n",
    "    \"https://www.berkeley.edu\",\n",
    "    \n",
    "    # Cloud services\n",
    "    \"https://www.dropbox.com\",\n",
    "    \"https://www.drive.google.com\",\n",
    "    \"https://www.onedrive.live.com\",\n",
    "    \n",
    "    # Developer tools\n",
    "    \"https://www.gitlab.com\",\n",
    "    \"https://www.bitbucket.org\",\n",
    "    \"https://www.docker.com\",\n",
    "    \"https://www.npmjs.com\",\n",
    "    \n",
    "    # Entertainment\n",
    "    \"https://www.netflix.com\",\n",
    "    \"https://www.spotify.com\",\n",
    "    \"https://www.twitch.tv\",\n",
    "    \"https://www.reddit.com\",\n",
    "    \n",
    "    # Add more as needed...\n",
    "]\n",
    "\n",
    "# Create legitimate dataframe\n",
    "legit_df = pd.DataFrame({\n",
    "    'url': legitimate_urls,\n",
    "    'label': 0\n",
    "})\n",
    "\n",
    "print(f\"✅ Added {len(legit_df)} legitimate URLs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Filter Active URLs\n",
    "**Goal:** Remove dead/offline URLs to save processing time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_url_active(url, timeout=5):\n",
    "    \"\"\"Check if URL is reachable\"\"\"\n",
    "    try:\n",
    "        # Add protocol if missing\n",
    "        if not url.startswith(('http://', 'https://')):\n",
    "            url = 'http://' + url\n",
    "        \n",
    "        response = requests.head(url, timeout=timeout, allow_redirects=True)\n",
    "        return response.status_code < 400\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def filter_active_urls(df, max_check=1000):\n",
    "    \"\"\"Filter dataframe to keep only active URLs\"\"\"\n",
    "    active_urls = []\n",
    "    \n",
    "    # Check first max_check URLs\n",
    "    check_df = df.head(max_check)\n",
    "    \n",
    "    for idx, row in tqdm(check_df.iterrows(), total=len(check_df), desc=\"Checking URLs\"):\n",
    "        if check_url_active(row['url']):\n",
    "            active_urls.append(row)\n",
    "    \n",
    "    return pd.DataFrame(active_urls)\n",
    "\n",
    "print(\"✅ URL filter function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter active phishing URLs (check first 500)\n",
    "print(\"Filtering active phishing URLs...\")\n",
    "active_phishing = filter_active_urls(train_df[train_df['label'] == 1], max_check=500)\n",
    "print(f\"Active phishing URLs: {len(active_phishing)}\")\n",
    "\n",
    "# All legitimate URLs are assumed active (popular sites)\n",
    "active_legitimate = legit_df\n",
    "print(f\"Legitimate URLs: {len(active_legitimate)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create Balanced Sample\n",
    "**Target:** 1000 phishing + 50 legitimate = 1050 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample balanced dataset\n",
    "n_phishing = min(1000, len(active_phishing))\n",
    "n_legitimate = len(active_legitimate)\n",
    "\n",
    "sampled_phishing = active_phishing.sample(n=n_phishing, random_state=42)\n",
    "sampled_legitimate = active_legitimate  # Use all\n",
    "\n",
    "# Combine\n",
    "balanced_df = pd.concat([sampled_phishing, sampled_legitimate], ignore_index=True)\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)  # Shuffle\n",
    "\n",
    "print(f\"\\n✅ Balanced dataset created:\")\n",
    "print(f\"Total: {len(balanced_df)} samples\")\n",
    "print(f\"Phishing: {sum(balanced_df['label'] == 1)}\")\n",
    "print(f\"Legitimate: {sum(balanced_df['label'] == 0)}\")\n",
    "\n",
    "# Save balanced dataset\n",
    "balanced_df.to_csv('../01_data/splits/balanced_train.csv', index=False)\n",
    "print(\"\\n✅ Saved to: 01_data/splits/balanced_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Process in Batches\n",
    "**Strategy:** Process 50 URLs at a time to avoid memory issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = DataPreprocessor(config)\n",
    "print(\"✅ Preprocessor initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process in batches\n",
    "BATCH_SIZE = 50\n",
    "total_samples = len(balanced_df)\n",
    "all_results = []\n",
    "\n",
    "print(f\"Processing {total_samples} URLs in batches of {BATCH_SIZE}...\\n\")\n",
    "\n",
    "for batch_start in range(0, total_samples, BATCH_SIZE):\n",
    "    batch_end = min(batch_start + BATCH_SIZE, total_samples)\n",
    "    batch_num = batch_start // BATCH_SIZE + 1\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Batch {batch_num}/{(total_samples + BATCH_SIZE - 1) // BATCH_SIZE}\")\n",
    "    print(f\"Processing samples {batch_start} to {batch_end}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Create temporary CSV for this batch\n",
    "    batch_df = balanced_df.iloc[batch_start:batch_end]\n",
    "    batch_csv = f'../01_data/splits/temp_batch_{batch_num}.csv'\n",
    "    batch_df.to_csv(batch_csv, index=False)\n",
    "    \n",
    "    # Process batch\n",
    "    try:\
