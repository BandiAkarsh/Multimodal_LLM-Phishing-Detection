# %load 06_notebooks/01_data_collection.ipynb
import pandas as pd
import numpy as np

# Load PhishTank data
phishTank_df = pd.read_csv('01_data/raw/phishtank.csv')
phishTank_df = phishTank_df[['url']]  # Only keep URL column
phishTank_df['label'] = 1  # phishing

# Load OpenPhish data
with open('01_data/raw/openphish.txt', 'r') as f:
    openphish_urls = [line.strip() for line in f if line.strip()]
openphish_df = pd.DataFrame({'url': openphish_urls})
openphish_df['label'] = 1  # phishing

# Combine datasets
combined_df = pd.concat([phishTank_df, openphish_df], ignore_index=True)
combined_df = combined_df.drop_duplicates(subset='url').reset_index(drop=True)

# Add legitimate URLs (you'll need to collect these)
# For now, let's create a small sample of legitimate URLs
legitimate_urls = [
    "https://www.google.com",
    "https://www.github.com",
    "https://www.microsoft.com",
    "https://www.apple.com",
    "https://www.amazon.com",
    "https://www.wikipedia.org",
    "https://www.stackoverflow.com",
    "https://www.cnn.com",
    "https://www.bbc.com",
    "https://www.nytimes.com"
]

legitimate_df = pd.DataFrame({'url': legitimate_urls})
legitimate_df['label'] = 0  # legitimate

# Combine all data
final_df = pd.concat([combined_df, legitimate_df], ignore_index=True)
final_df = final_df.sample(frac=1, random_state=42).reset_index(drop=True)

# Save to raw directory
final_df.to_csv('01_data/raw/combined_dataset.csv', index=False)
print(f'Dataset saved: {len(final_df)} total samples')
print(f'Phishing: {len(final_df[final_df["label"] == 1])}')
print(f'Legitimate: {len(final_df[final_df["label"] == 0])}')
